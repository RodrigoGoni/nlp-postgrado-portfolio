# Portfolio de Procesamiento de Lenguaje Natural

> **EspecializaciÃ³n en Procesamiento de Lenguaje Natural**  
> ColecciÃ³n completa de desafÃ­os prÃ¡cticos implementando tÃ©cnicas avanzadas de NLP con Deep Learning

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)](https://pytorch.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0%2B-orange)](https://www.tensorflow.org/)

---

## Ãndice

- [DescripciÃ³n General](#descripciÃ³n-general)
- [DesafÃ­os Implementados](#desafÃ­os-implementados)
  - [DesafÃ­o 1: Bag of Words](#desafÃ­o-1-bag-of-words)
  - [DesafÃ­o 2: Custom Embeddings](#desafÃ­o-2-custom-embeddings)
  - [DesafÃ­o 3: Modelo de Lenguaje](#desafÃ­o-3-modelo-de-lenguaje)
  - [DesafÃ­o 4: Traductor Seq2Seq](#desafÃ­o-4-traductor-seq2seq)
- [TecnologÃ­as Utilizadas](#tecnologÃ­as-utilizadas)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Resultados Destacados](#resultados-destacados)
- [Autor](#autor)
- [Agradecimientos](#agradecimientos)

---

## DescripciÃ³n General

Este repositorio consolida los **cuatro desafÃ­os principales** del programa de postgrado en **Procesamiento de Lenguaje Natural**, abarcando desde tÃ©cnicas clÃ¡sicas de vectorizaciÃ³n hasta arquitecturas neuronales avanzadas como Seq2Seq con atenciÃ³n.

Cada desafÃ­o estÃ¡ implementado como un **submÃ³dulo Git independiente**, permitiendo navegaciÃ³n modular mientras se mantiene un portfolio unificado para presentaciÃ³n acadÃ©mica y profesional.

### Competencias Desarrolladas

- TÃ©cnicas ClÃ¡sicas de NLP: TF-IDF, Bag of Words, clasificaciÃ³n por similitud
- Word Embeddings: Word2Vec (Skip-gram, CBOW), embeddings personalizados
- Arquitecturas Recurrentes: RNN, LSTM, GRU con capas apiladas
- Modelos de Secuencia: Encoder-Decoder, Seq2Seq, traducciÃ³n automÃ¡tica
- GeneraciÃ³n de Texto: Greedy search, Beam search, Sampling con temperatura
- OptimizaciÃ³n: Early stopping, Dropout, L2 regularization, Adam optimizer

---

## DesafÃ­os Implementados

### DesafÃ­o 1: Bag of Words

**TÃ©cnicas clÃ¡sicas de NLP y clasificaciÃ³n de documentos**

**Repositorio**: [`bag-of-words-npl`](https://github.com/RodrigoGoni/bag-of-words-npl)

#### DescripciÃ³n
ImplementaciÃ³n completa de mÃ©todos tradicionales de vectorizaciÃ³n y clasificaciÃ³n de textos utilizando el dataset **20 Newsgroups** (20,000 documentos en 20 categorÃ­as).

#### CaracterÃ­sticas Principales
- VectorizaciÃ³n TF-IDF para representaciÃ³n de documentos
- ClasificaciÃ³n por prototipos (Zero-shot learning con vecino mÃ¡s cercano)
- Modelos NaÃ¯ve Bayes: MultinomialNB y ComplementNB optimizados
- AnÃ¡lisis de co-ocurrencia mediante matrices tÃ©rmino-documento
- CÃ¡lculo de similaridad entre documentos y palabras

#### Resultados
| Modelo | F1-Score Macro |
|--------|---------------|
| ClasificaciÃ³n por Prototipos | 0.5050 |
| MultinomialNB | 0.6833 |
| ComplementNB | **0.6950** |

#### Stack TecnolÃ³gico
`scikit-learn` Â· `NLTK` Â· `NumPy` Â· `Pandas` Â· `Matplotlib`

---

### DesafÃ­o 2: Custom Embeddings

**Entrenamiento de embeddings personalizados con Word2Vec**

**Repositorio**: [`customs_embeddings`](https://github.com/RodrigoGoni/customs_embeddings)

#### DescripciÃ³n
CreaciÃ³n de vectores de embeddings desde cero utilizando **Gensim** sobre datasets personalizados: letras de canciones de diversos artistas y textos religiosos (Evangelio de Juan).

#### CaracterÃ­sticas Principales
- Word2Vec con Skip-gram y CBOW
- Preprocesamiento con NLTK: tokenizaciÃ³n, stopwords, stemming
- VisualizaciÃ³n de espacios semÃ¡nticos con PCA/t-SNE
- AnÃ¡lisis de similitudes y analogÃ­as entre palabras
- Web scraping de fuentes de texto con BeautifulSoup
- ExportaciÃ³n a TensorFlow Projector (vectors.tsv/labels.tsv)

#### Datasets Utilizados
- Letras de canciones de mÃºltiples artistas (corpus principal)
- Evangelio de Juan - Biblia de JerusalÃ©n (corpus comparativo)

#### Stack TecnolÃ³gico
`Gensim` Â· `NLTK` Â· `BeautifulSoup` Â· `Matplotlib` Â· `Seaborn`

---

### DesafÃ­o 3: Modelo de Lenguaje

**GeneraciÃ³n de texto con redes recurrentes a nivel de carÃ¡cter**

**Repositorio**: [`chatbot`](https://github.com/RodrigoGoni/chatbot)

#### DescripciÃ³n
ImplementaciÃ³n de modelos de lenguaje basados en RNNs para **generaciÃ³n de texto en espaÃ±ol** a nivel de carÃ¡cter, entrenados sobre un corpus de literatura clÃ¡sica espaÃ±ola del Proyecto Gutenberg (~7.5M caracteres de 8 libros).

#### CaracterÃ­sticas Principales
- Arquitecturas implementadas: SimpleRNN, LSTM, GRU (bÃ¡sicas y avanzadas con capas apiladas)
- Estrategias de generaciÃ³n: Greedy Search, Beam Search, Sampling con temperatura
- Anti-overfitting: Early stopping, Dropout (0.5), Weight Decay (L2)
- MÃ©tricas de evaluaciÃ³n: Loss, Accuracy, Perplexity
- AnÃ¡lisis lingÃ¼Ã­stico con SpaCy (coherencia gramatical)
- GestiÃ³n con Git LFS para modelos y datasets grandes

#### Resultados
| Modelo | Val Perplexity | Val Accuracy | Calidad GeneraciÃ³n |
|--------|---------------|--------------|-------------------|
| SimpleRNN | 4.23 | 0.52 | BÃ¡sica |
| GRU | 3.87 | 0.57 | Buena |
| LSTM | **3.51** | **0.60** | **Excelente (mejor estructura gramatical)** |

#### Stack TecnolÃ³gico
`TensorFlow/Keras` Â· `SpaCy` Â· `NumPy` Â· `Matplotlib` Â· `Git LFS`

---

### DesafÃ­o 4: Traductor Seq2Seq

**TraducciÃ³n automÃ¡tica InglÃ©sâ†’EspaÃ±ol con arquitectura Encoder-Decoder**

**Repositorio**: [`seq2seq-translator`](https://github.com/RodrigoGoni/seq2seq-translator)

#### DescripciÃ³n
Sistema de traducciÃ³n neuronal basado en **LSTM bidireccionales** con arquitectura sequence-to-sequence, utilizando embeddings pre-entrenados GloVe y optimizaciÃ³n inteligente de hiperparÃ¡metros.

#### CaracterÃ­sticas Principales
- Encoder-Decoder con LSTM de 2 capas
- Embeddings GloVe 50d para inglÃ©s (congelados, ~600k parÃ¡metros)
- Embeddings entrenables para espaÃ±ol (~1.25M parÃ¡metros)
- SelecciÃ³n inteligente de hiperparÃ¡metros basada en percentiles del dataset:
  - Vocabulario: Cobertura del 98% (P98)
  - Longitudes de secuencia: Percentil 98
- TÃ©cnicas de regularizaciÃ³n:
  - Early Stopping (patience=7)
  - Dropout (0.5)
  - L2 Regularization (weight_decay=1e-5)
- Dos variantes: 128 y 256 neuronas LSTM
- Guardado/carga de modelos entrenados (.pth)

#### Dataset
- Fuente: TensorFlow spa-eng
- TamaÃ±o: 118,964 pares de oraciones
- Split: 80% train / 20% validaciÃ³n

#### Arquitectura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ENCODER (InglÃ©s)                â”‚
â”‚  Embedding GloVe 50d â†’ LSTM(2 capas)    â”‚
â”‚  Output: Hidden State [2, batch, 128/256]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DECODER (EspaÃ±ol)               â”‚
â”‚  Embedding Trainable â†’ LSTM(2 capas)    â”‚
â”‚  FC â†’ Softmax(vocab_size=25k)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Resultados
| Modelo | ParÃ¡metros Entrenables | Convergencia |
|--------|----------------------|--------------|
| LSTM-128 | ~4.9M | Early stopping aplicado |
| LSTM-256 | ~4.9M | Mayor capacidad representacional |

#### Stack TecnolÃ³gico
`PyTorch` Â· `torchinfo` Â· `NumPy` Â· `Matplotlib` Â· `GloVe Embeddings`

---

## TecnologÃ­as Utilizadas

### Frameworks de Deep Learning
- **PyTorch** 2.0+ (DesafÃ­os 4)
- **TensorFlow/Keras** 2.0+ (DesafÃ­o 3)

### NLP Libraries
- **Gensim** (Word2Vec, embeddings)
- **NLTK** (preprocesamiento)
- **SpaCy** (anÃ¡lisis lingÃ¼Ã­stico)
- **scikit-learn** (clasificaciÃ³n clÃ¡sica)

### VisualizaciÃ³n & AnÃ¡lisis
- **Matplotlib** / **Seaborn**
- **TensorFlow Projector** (visualizaciÃ³n de embeddings)

### Herramientas de Desarrollo
- **Jupyter Notebook** / **VS Code**
- **Git LFS** (gestiÃ³n de archivos grandes)
- **BeautifulSoup** (web scraping)

---

## InstalaciÃ³n

**Consulta la [GuÃ­a de InstalaciÃ³n Completa](INSTALL.md)** para instrucciones detalladas.

**InstalaciÃ³n rÃ¡pida:**
```bash
git clone --recurse-submodules https://github.com/RodrigoGoni/nlp-postgrado-portfolio.git
cd nlp-postgrado-portfolio
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

---

## Estructura del Proyecto

```
nlp-postgrado-portfolio/
â”‚
â”œâ”€â”€ README.md                           # Este archivo (portfolio principal)
â”œâ”€â”€ .gitignore                          # Exclusiones de Git
â”œâ”€â”€ .gitmodules                         # ConfiguraciÃ³n de submÃ³dulos
â”œâ”€â”€ setup.sh                            # Script de inicializaciÃ³n automÃ¡tica
â”‚
â”œâ”€â”€ desafio1-bag-of-words/              # SubmÃ³dulo â†’ bag-of-words-npl
â”‚   â”œâ”€â”€ main.ipynb
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ desafio2-custom-embeddings/         # SubmÃ³dulo â†’ customs_embeddings
â”‚   â”œâ”€â”€ main.ipynb
â”‚   â”œâ”€â”€ songs_dataset/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ desafio3-chatbot/                   # SubmÃ³dulo â†’ chatbot
â”‚   â”œâ”€â”€ desafio3.ipynb
â”‚   â”œâ”€â”€ corpus_espanol.txt (Git LFS)
â”‚   â”œâ”€â”€ model_*.keras (Git LFS)
â”‚   â”œâ”€â”€ requirement.txt
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ desafio4-seq2seq-translator/        # SubmÃ³dulo â†’ seq2seq-translator
    â”œâ”€â”€ traductor_simplificado.ipynb
    â”œâ”€â”€ torch_helpers.py
    â””â”€â”€ README.md
```

---

## Resultados Destacados

### ProgresiÃ³n de Complejidad

| DesafÃ­o | TÃ©cnica | Nivel | Resultado Clave |
|---------|---------|-------|-----------------|
| 1 | Bag of Words + TF-IDF | BÃ¡sico | F1=0.695 (ComplementNB) |
| 2 | Word2Vec (Gensim) | Intermedio | Embeddings personalizados coherentes |
| 3 | LSTM a nivel de carÃ¡cter | Avanzado | Perplexity=3.51, generaciÃ³n coherente |
| 4 | Seq2Seq con GloVe | Experto | TraducciÃ³n ENâ†’ES con ~4.9M params |

---

## Autor

**Rodrigo GoÃ±i**

ğŸ“§ Email: [tu-email@ejemplo.com](mailto:tu-email@ejemplo.com)  
ğŸ”— GitHub: [@RodrigoGoni](https://github.com/RodrigoGoni)  
ğŸ’¼ LinkedIn: [Tu perfil](https://linkedin.com/in/tu-perfil)

---

## Licencia

Este proyecto estÃ¡ bajo la licencia **Apache 2.0**. Consulta cada submÃ³dulo para licencias especÃ­ficas.

---

## Agradecimientos

### Docentes

Agradezco profundamente a los docentes que han guiado este proceso de formaciÃ³n:

- **Dr. Rodrigo Cardenas Szigety** (2022-actual)
- **Dr. NicolÃ¡s Vattuone** (2025-actual)
- **Esp. Ing. HernÃ¡n Contigiani** (2021-2022)



