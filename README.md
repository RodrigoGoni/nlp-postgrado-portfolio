# ğŸ§  Portfolio de Procesamiento de Lenguaje Natural

> **EspecializaciÃ³n en Procesamiento de Lenguaje Natural**  
> ColecciÃ³n completa de desafÃ­os prÃ¡cticos implementando tÃ©cnicas avanzadas de NLP con Deep Learning

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)](https://pytorch.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0%2B-orange)](https://www.tensorflow.org/)

---

## ğŸ“š Ãndice

- [DescripciÃ³n General](#-descripciÃ³n-general)
- [DesafÃ­os Implementados](#-desafÃ­os-implementados)
  - [DesafÃ­o 1: Bag of Words](#1ï¸âƒ£-desafÃ­o-1-bag-of-words)
  - [DesafÃ­o 2: Custom Embeddings](#2ï¸âƒ£-desafÃ­o-2-custom-embeddings)
  - [DesafÃ­o 3: Modelo de Lenguaje](#3ï¸âƒ£-desafÃ­o-3-modelo-de-lenguaje)
  - [DesafÃ­o 4: Traductor Seq2Seq](#4ï¸âƒ£-desafÃ­o-4-traductor-seq2seq)
- [TecnologÃ­as Utilizadas](#-tecnologÃ­as-utilizadas)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Resultados Destacados](#-resultados-destacados)
- [Autor](#-autor)

---

## ğŸ¯ DescripciÃ³n General

Este repositorio consolida los **cuatro desafÃ­os principales** del programa de postgrado en **Procesamiento de Lenguaje Natural**, abarcando desde tÃ©cnicas clÃ¡sicas de vectorizaciÃ³n hasta arquitecturas neuronales avanzadas como Seq2Seq con atenciÃ³n.

Cada desafÃ­o estÃ¡ implementado como un **submÃ³dulo Git independiente**, permitiendo navegaciÃ³n modular mientras se mantiene un portfolio unificado para presentaciÃ³n acadÃ©mica y profesional.

### ğŸ“ Competencias Desarrolladas

- **TÃ©cnicas ClÃ¡sicas de NLP**: TF-IDF, Bag of Words, clasificaciÃ³n por similitud
- **Word Embeddings**: Word2Vec (Skip-gram, CBOW), embeddings personalizados
- **Arquitecturas Recurrentes**: RNN, LSTM, GRU con capas apiladas
- **Modelos de Secuencia**: Encoder-Decoder, Seq2Seq, traducciÃ³n automÃ¡tica
- **GeneraciÃ³n de Texto**: Greedy search, Beam search, Sampling con temperatura
- **OptimizaciÃ³n**: Early stopping, Dropout, L2 regularization, Adam optimizer

---

## ğŸš€ DesafÃ­os Implementados

### 1ï¸âƒ£ DesafÃ­o 1: Bag of Words

**TÃ©cnicas clÃ¡sicas de NLP y clasificaciÃ³n de documentos**

ğŸ“‚ **Repositorio**: [`bag-of-words-npl`](https://github.com/RodrigoGoni/bag-of-words-npl)

#### DescripciÃ³n
ImplementaciÃ³n completa de mÃ©todos tradicionales de vectorizaciÃ³n y clasificaciÃ³n de textos utilizando el dataset **20 Newsgroups** (20,000 documentos en 20 categorÃ­as).

#### CaracterÃ­sticas Principales
- âœ… **VectorizaciÃ³n TF-IDF** para representaciÃ³n de documentos
- âœ… **ClasificaciÃ³n por prototipos** (Zero-shot learning con vecino mÃ¡s cercano)
- âœ… **Modelos NaÃ¯ve Bayes**: MultinomialNB y ComplementNB optimizados
- âœ… **AnÃ¡lisis de co-ocurrencia** mediante matrices tÃ©rmino-documento
- âœ… **CÃ¡lculo de similaridad** entre documentos y palabras

#### Resultados
| Modelo | F1-Score Macro |
|--------|---------------|
| ClasificaciÃ³n por Prototipos | 0.5050 |
| MultinomialNB | 0.6833 |
| **ComplementNB** | **0.6950** â­ |

#### Stack TecnolÃ³gico
`scikit-learn` Â· `NLTK` Â· `NumPy` Â· `Pandas` Â· `Matplotlib`

---

### 2ï¸âƒ£ DesafÃ­o 2: Custom Embeddings

**Entrenamiento de embeddings personalizados con Word2Vec**

ğŸ“‚ **Repositorio**: [`customs_embeddings`](https://github.com/RodrigoGoni/customs_embeddings)

#### DescripciÃ³n
CreaciÃ³n de vectores de embeddings desde cero utilizando **Gensim** sobre datasets personalizados: letras de canciones de diversos artistas y textos religiosos (Evangelio de Juan).

#### CaracterÃ­sticas Principales
- âœ… **Word2Vec con Skip-gram y CBOW**
- âœ… **Preprocesamiento con NLTK**: tokenizaciÃ³n, stopwords, stemming
- âœ… **VisualizaciÃ³n de espacios semÃ¡nticos** con PCA/t-SNE
- âœ… **AnÃ¡lisis de similitudes** y analogÃ­as entre palabras
- âœ… **Web scraping** de fuentes de texto con BeautifulSoup
- âœ… **ExportaciÃ³n a TensorFlow Projector** (vectors.tsv/labels.tsv)

#### Datasets Utilizados
- ğŸ“ Letras de canciones de mÃºltiples artistas (corpus principal)
- ğŸ“– Evangelio de Juan - Biblia de JerusalÃ©n (corpus comparativo)

#### Stack TecnolÃ³gico
`Gensim` Â· `NLTK` Â· `BeautifulSoup` Â· `Matplotlib` Â· `Seaborn`

---

### 3ï¸âƒ£ DesafÃ­o 3: Modelo de Lenguaje

**GeneraciÃ³n de texto con redes recurrentes a nivel de carÃ¡cter**

ğŸ“‚ **Repositorio**: [`chatbot`](https://github.com/RodrigoGoni/chatbot)

#### DescripciÃ³n
ImplementaciÃ³n de modelos de lenguaje basados en RNNs para **generaciÃ³n de texto en espaÃ±ol** a nivel de carÃ¡cter, entrenados sobre un corpus de literatura clÃ¡sica espaÃ±ola del Proyecto Gutenberg (~7.5M caracteres de 8 libros).

#### CaracterÃ­sticas Principales
- âœ… **Arquitecturas implementadas**: SimpleRNN, LSTM, GRU (bÃ¡sicas y avanzadas con capas apiladas)
- âœ… **Estrategias de generaciÃ³n**: Greedy Search, Beam Search, Sampling con temperatura
- âœ… **Anti-overfitting**: Early stopping, Dropout (0.5), Weight Decay (L2)
- âœ… **MÃ©tricas de evaluaciÃ³n**: Loss, Accuracy, Perplexity
- âœ… **AnÃ¡lisis lingÃ¼Ã­stico** con SpaCy (coherencia gramatical)
- âœ… **GestiÃ³n con Git LFS** para modelos y datasets grandes

#### Resultados
| Modelo | Val Perplexity | Val Accuracy | Calidad GeneraciÃ³n |
|--------|---------------|--------------|-------------------|
| SimpleRNN | 4.23 | 0.52 | BÃ¡sica |
| GRU | 3.87 | 0.57 | Buena |
| **LSTM** | **3.51** â­ | **0.60** â­ | **Excelente (mejor estructura gramatical)** |

#### Stack TecnolÃ³gico
`TensorFlow/Keras` Â· `SpaCy` Â· `NumPy` Â· `Matplotlib` Â· `Git LFS`

---

### 4ï¸âƒ£ DesafÃ­o 4: Traductor Seq2Seq

**TraducciÃ³n automÃ¡tica InglÃ©sâ†’EspaÃ±ol con arquitectura Encoder-Decoder**

ğŸ“‚ **Repositorio**: [`seq2seq-translator`](https://github.com/RodrigoGoni/seq2seq-translator)

#### DescripciÃ³n
Sistema de traducciÃ³n neuronal basado en **LSTM bidireccionales** con arquitectura sequence-to-sequence, utilizando embeddings pre-entrenados GloVe y optimizaciÃ³n inteligente de hiperparÃ¡metros.

#### CaracterÃ­sticas Principales
- âœ… **Encoder-Decoder con LSTM** de 2 capas
- âœ… **Embeddings GloVe 50d** para inglÃ©s (congelados, ~600k parÃ¡metros)
- âœ… **Embeddings entrenables** para espaÃ±ol (~1.25M parÃ¡metros)
- âœ… **SelecciÃ³n inteligente de hiperparÃ¡metros** basada en percentiles del dataset:
  - Vocabulario: Cobertura del 98% (P98)
  - Longitudes de secuencia: Percentil 98
- âœ… **TÃ©cnicas de regularizaciÃ³n**:
  - Early Stopping (patience=7)
  - Dropout (0.5)
  - L2 Regularization (weight_decay=1e-5)
- âœ… **Dos variantes**: 128 y 256 neuronas LSTM
- âœ… **Guardado/carga de modelos** entrenados (.pth)

#### Dataset
- ğŸ“Š **Fuente**: TensorFlow spa-eng
- ğŸ“Š **TamaÃ±o**: 118,964 pares de oraciones
- ğŸ“Š **Split**: 80% train / 20% validaciÃ³n

#### Arquitectura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ENCODER (InglÃ©s)                â”‚
â”‚  Embedding GloVe 50d â†’ LSTM(2 capas)    â”‚
â”‚  Output: Hidden State [2, batch, 128/256]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DECODER (EspaÃ±ol)               â”‚
â”‚  Embedding Trainable â†’ LSTM(2 capas)    â”‚
â”‚  FC â†’ Softmax(vocab_size=25k)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Resultados
| Modelo | ParÃ¡metros Entrenables | Convergencia |
|--------|----------------------|--------------|
| LSTM-128 | ~4.9M | Early stopping aplicado |
| LSTM-256 | ~4.9M | Mayor capacidad representacional |

#### Stack TecnolÃ³gico
`PyTorch` Â· `torchinfo` Â· `NumPy` Â· `Matplotlib` Â· `GloVe Embeddings`

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Frameworks de Deep Learning
- **PyTorch** 2.0+ (DesafÃ­os 4)
- **TensorFlow/Keras** 2.0+ (DesafÃ­o 3)

### NLP Libraries
- **Gensim** (Word2Vec, embeddings)
- **NLTK** (preprocesamiento)
- **SpaCy** (anÃ¡lisis lingÃ¼Ã­stico)
- **scikit-learn** (clasificaciÃ³n clÃ¡sica)

### VisualizaciÃ³n & AnÃ¡lisis
- **Matplotlib** / **Seaborn**
- **TensorFlow Projector** (visualizaciÃ³n de embeddings)

### Herramientas de Desarrollo
- **Jupyter Notebook** / **VS Code**
- **Git LFS** (gestiÃ³n de archivos grandes)
- **BeautifulSoup** (web scraping)

---

## ğŸ“¥ InstalaciÃ³n

### Prerrequisitos
- Python 3.8+
- Git 2.0+
- Git LFS (para el DesafÃ­o 3)

### Clonar el Portfolio Completo con SubmÃ³dulos

```bash
# Clonar repositorio principal
git clone --recurse-submodules https://github.com/RodrigoGoni/nlp-postgrado-portfolio.git
cd nlp-postgrado-portfolio

# Si ya clonaste sin submÃ³dulos, inicialÃ­zalos:
git submodule update --init --recursive
```

### Instalar Dependencias

#### OpciÃ³n 1: Entorno virtual unificado (recomendado para exploraciÃ³n)
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar todas las dependencias
pip install -r desafio1-bag-of-words/requirements.txt
pip install -r desafio2-custom-embeddings/requirements.txt
pip install -r desafio3-chatbot/requirement.txt
pip install -r desafio4-seq2seq-translator/requirements.txt

# Instalar SpaCy (para DesafÃ­o 3)
python -m spacy download es_core_news_sm
```

#### OpciÃ³n 2: Entornos separados (recomendado para desarrollo)
```bash
# Navegar a cada desafÃ­o e instalar individualmente
cd desafio1-bag-of-words
pip install -r requirements.txt
cd ..
# Repetir para cada desafÃ­o
```

---

## ğŸ“‚ Estructura del Proyecto

```
nlp-postgrado-portfolio/
â”‚
â”œâ”€â”€ README.md                           # Este archivo (portfolio principal)
â”œâ”€â”€ .gitignore                          # Exclusiones de Git
â”œâ”€â”€ .gitmodules                         # ConfiguraciÃ³n de submÃ³dulos
â”œâ”€â”€ setup.sh                            # Script de inicializaciÃ³n automÃ¡tica
â”‚
â”œâ”€â”€ desafio1-bag-of-words/              # SubmÃ³dulo â†’ bag-of-words-npl
â”‚   â”œâ”€â”€ main.ipynb
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ desafio2-custom-embeddings/         # SubmÃ³dulo â†’ customs_embeddings
â”‚   â”œâ”€â”€ main.ipynb
â”‚   â”œâ”€â”€ songs_dataset/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ desafio3-chatbot/                   # SubmÃ³dulo â†’ chatbot
â”‚   â”œâ”€â”€ desafio3.ipynb
â”‚   â”œâ”€â”€ corpus_espanol.txt (Git LFS)
â”‚   â”œâ”€â”€ model_*.keras (Git LFS)
â”‚   â”œâ”€â”€ requirement.txt
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ desafio4-seq2seq-translator/        # SubmÃ³dulo â†’ seq2seq-translator
    â”œâ”€â”€ traductor_simplificado.ipynb
    â”œâ”€â”€ torch_helpers.py
    â””â”€â”€ README.md
```

---

## ğŸ“Š Resultados Destacados

### ProgresiÃ³n de Complejidad

| DesafÃ­o | TÃ©cnica | Nivel | Resultado Clave |
|---------|---------|-------|-----------------|
| 1 | Bag of Words + TF-IDF | BÃ¡sico | F1=0.695 (ComplementNB) |
| 2 | Word2Vec (Gensim) | Intermedio | Embeddings personalizados coherentes |
| 3 | LSTM a nivel de carÃ¡cter | Avanzado | Perplexity=3.51, generaciÃ³n coherente |
| 4 | Seq2Seq con GloVe | Experto | TraducciÃ³n ENâ†’ES con ~4.9M params |

### MÃ©tricas Comparativas

```
ClasificaciÃ³n de Textos (DesafÃ­o 1):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
MultinomialNB    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 68.3%
ComplementNB     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘ 69.5% â­

Modelos de Lenguaje (DesafÃ­o 3):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
SimpleRNN        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 52.0%
GRU              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘â–‘â–‘ 57.0%
LSTM             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 60.0% â­
```

---

## ğŸ‘¤ Autor

**Rodrigo GoÃ±i**

ğŸ“§ Email: [tu-email@ejemplo.com](mailto:tu-email@ejemplo.com)  
ğŸ”— GitHub: [@RodrigoGoni](https://github.com/RodrigoGoni)  
ğŸ’¼ LinkedIn: [Tu perfil](https://linkedin.com/in/tu-perfil)

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia **Apache 2.0**. Consulta cada submÃ³dulo para licencias especÃ­ficas.

---

## ğŸ™ Agradecimientos

- **Programa de Postgrado en NLP** - Universidad/InstituciÃ³n
- **Datasets**: 20 Newsgroups, TensorFlow spa-eng, Proyecto Gutenberg
- **Embeddings**: Stanford GloVe
- **Comunidad Open Source**: PyTorch, TensorFlow, Gensim, NLTK, SpaCy

---

## ğŸ”„ Estado del Proyecto

![Estado](https://img.shields.io/badge/Estado-Completo-success)
![Ãšltima actualizaciÃ³n](https://img.shields.io/badge/Ãšltima%20actualizaciÃ³n-Diciembre%202025-blue)

**Todos los desafÃ­os estÃ¡n completamente implementados y documentados** âœ…

---

<div align="center">

### â­ Si este portfolio te resulta Ãºtil, considera darle una estrella en GitHub

**Desarrollado con ğŸ§  por Rodrigo GoÃ±i**

</div>
